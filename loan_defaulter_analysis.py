# -*- coding: utf-8 -*-
"""Loan_Defaulter_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SPyJVOxpWw1Vrz0NLz65OBJOa6pi-9F_
"""

#from google.colab import drive
#drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import pyplot
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

# Loading dataset
data = pd.read_csv("/content/drive/MyDrive/Loan_default.csv")
data

data.info()

# Exploratory Data Analysis

data.shape

data.EmploymentType.unique()

data.isnull().sum()   # there is no null values

# Creating categorical data and numerical data
categorical_df=data[['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose','NumCreditLines', 'HasCoSigner', 'Default']]
numerical_df=data[['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'InterestRate', 'LoanTerm', 'DTIRatio']]
numerical_df

# Checking for data error
for variable in categorical_df:
    print(data[variable].value_counts())
    print("-"*50)

# Our dataset is in clean format there is no error.

# Checking for outliers
# Assuming numerical_df is your DataFrame
for column in numerical_df.columns:
    plt.figure()
    plt.title(column)
    numerical_df.boxplot(column)
    plt.show()

"""**Our data is outliers free.**

**Data Visualization**
"""

numerical_df.hist( figsize = (22, 20) )
plt.show()

# histograms of the variables
#data.hist()
#pyplot.show()

plt.figure(figsize=(16,5))
plt.subplot(1,2,1)
sns.distplot(numerical_df['Age'])

# Create a scatter plot matrix
#sns.set(style="ticks")
#sns.pairplot(numerical_df)
#plt.show()

data['Default'].value_counts()

fig, ax = plt.subplots( figsize = (6,6) )
#fig, ax = plt.subplots()
ax.bar(data['Education'].value_counts().index, data['Education'].value_counts().values)
plt.show()

x = data['LoanPurpose'].value_counts()
plt.pie(x.values,
        labels=x.index,
        autopct='%1.1f%%')
plt.show()

data.head(1)

fig, ax = plt.subplots()
sns.countplot(x='Education', hue='Default', ax=ax, data=data)

fig, ax = plt.subplots()
sns.countplot(x='MaritalStatus', hue='Default', ax=ax, data=data)

data.head(1)

plt.figure(figsize=(15, 5))
sns.barplot(x=data['HasMortgage'], y=data['Income'])

sns.kdeplot(data.Age)



df=data.copy()
df.head(2)

#df.drop("Age",axis=1,inplace=True)
#df.drop("Income",axis=1,inplace=True)
#df.drop("LoanAmount",axis=1,inplace=True)
#df.drop("CreditScore",axis=1,inplace=True)
#df.drop("MonthsEmployed",axis=1,inplace=True)
#df.drop("InterestRate",axis=1,inplace=True)
#df.drop("LoanTerm",axis=1,inplace=True)
#df.drop("DTIRatio",axis=1,inplace=True)

"""Normalising the Data"""

#Normalizing the data

from sklearn import preprocessing

d = preprocessing.normalize(numerical_df)
scaled_df = pd.DataFrame(d, columns=numerical_df.columns)
print(scaled_df)

scaled_df.info()

#data_log.hist(figsize = (22, 20))
#plt.show()

# histograms of the variables
scaled_df.hist()
plt.show()

sns.kdeplot(scaled_df)



# Removing Skewness by using Quantile Transformer

from sklearn.preprocessing import QuantileTransformer
# quantile transform the raw data
quantile = QuantileTransformer(output_distribution='normal')
data_trans = quantile.fit_transform(scaled_df)
data_trans=pd.DataFrame(data_trans,columns=numerical_df.columns)
data_trans
#data_trans.info()



# histograms of the variables
data_trans.hist()
plt.show()



sns.kdeplot(data_trans)

df=data.copy()
df.drop("Age",axis=1,inplace=True)
df.drop("Income",axis=1,inplace=True)
df.drop("LoanAmount",axis=1,inplace=True)
df.drop("CreditScore",axis=1,inplace=True)
df.drop("MonthsEmployed",axis=1,inplace=True)
df.drop("InterestRate",axis=1,inplace=True)
df.drop("LoanTerm",axis=1,inplace=True)
df.drop("DTIRatio",axis=1,inplace=True)

data_1=pd.concat([df,data_trans],axis=1,join='inner')
data_1

data_trans.info()

# Changing the name of normalised data
#data_1.rename(columns = {0:'Age',1:'Income',2:'LoanAmount',3:'CreditScore',4:'MonthsEmployed',5:'InterestRate',6:'LoanTerm',7:'DTIRatio'}, inplace = True)
#data_1

"""**Feature Engineering**

"""

#from sklearn.preprocessing import LabelEncoder

# Import label encoder
#from sklearn import preprocessing

# label_encoder object knows
#label_encoder = preprocessing.LabelEncoder()

# Encode labels in column 'species'.
#data_1['Education']= label_encoder.fit_transform(data_1['Education'])

data_1.head(2)

from sklearn.preprocessing import OneHotEncoder

# One hot Encoding
data_1 = pd.get_dummies(data_1,columns=list(['Education','EmploymentType','MaritalStatus','HasMortgage','HasDependents','LoanPurpose','HasCoSigner']),dtype=int)
data_1

"""**Feature Scaling**"""

#data.info()
data_1.drop(columns=['LoanID'], inplace=True)



from sklearn import preprocessing

# Create a MinMaxScaler object
scaler = preprocessing.MinMaxScaler()

# Fit the scaler to the data
scaler.fit(data_1)

# Transform the data using the scaler
normalized_data = scaler.fit_transform(data_1)
data_nor=pd.DataFrame(normalized_data,columns=data_1.columns)
data_nor

# Installing Pycaret library
#!pip install pycaret[full]
#!pip install --upgrade pandas "dask[complete]"
#!pip install --upgrade pandas
!pip install --upgrade pycaret

from pycaret.classification import *

reg = setup(data=data_1,
    target='Default',
    train_size=0.8,
    session_id=123,
    normalize=True
)

best = compare_models()

(best)



"""**Splitting the data**"""

# creating independent and dependent variable

X=data_nor.drop(columns=['Default'],axis=1)
y=data_nor['Default']
y

# applying oversampling on imbalance class

from imblearn.over_sampling import RandomOverSampler


ros = RandomOverSampler(
    sampling_strategy='auto',
    random_state=0,
)
X_res, y_res = ros.fit_resample(X, y)

"""**Feature Selection using Stratified K fold**"""

from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# Create a StratifiedKFold object
kfold = StratifiedKFold(n_splits=5, shuffle=True)

# Initialize the SelectKBest object
selector = SelectKBest(chi2, k=25)

# Perform stratified k fold cross validation
for train_index, test_index in kfold.split(X_res, y_res):
    # Split the data into training and testing sets
    X_train, X_test = X_res.iloc[train_index], X_res.iloc[test_index]
    y_train, y_test = y_res.iloc[train_index], y_res.iloc[test_index]

    # Fit the selector to the training data
    selector.fit(X_train, y_train)

    # Transform the test data
    X_test_transformed = selector.transform(X_test)

# Get the selected features
selected_features = selector.get_support(indices=True)

# Print the selected features
print('Selected features:', X.columns[selected_features])

data= data_nor[['NumCreditLines', 'Age', 'Income', 'LoanAmount', 'MonthsEmployed',
       'InterestRate', "Education_Bachelor's", 'Education_High School',
       "Education_Master's", 'Education_PhD', 'EmploymentType_Full-time',
       'EmploymentType_Unemployed', 'MaritalStatus_Divorced',
       'MaritalStatus_Married', 'MaritalStatus_Single', 'HasMortgage_No',
       'HasMortgage_Yes', 'HasDependents_No', 'HasDependents_Yes',
       'LoanPurpose_Business', 'LoanPurpose_Home', 'HasCoSigner_No',
       'HasCoSigner_Yes','Default','EmploymentType_Part-time','CreditScore']]
data

"""**Splitting the data**"""

# creating independent and dependent variable

X=data.drop(columns=['Default'],axis=1)
y=data['Default']
X

from sklearn.model_selection import train_test_split

# Split the data into train and test data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42, test_size=0.2)

X_train.shape





"""**Modeling**"""

from sklearn.ensemble import GradientBoostingClassifier

# Create a GradientBoostingClassifier object
clf = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)

# Fit the classifier to the training data
model=clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = clf.predict(X_test)

# Evaluate the classifier's performance
accuracy = clf.score(X_test, y_test)
print("Accuracy:", accuracy)

from sklearn import metrics

print('Training Accuracy : ',
      metrics.accuracy_score(y_train,
                             model.predict(X_train))*100)
print('Validation Accuracy : ',
      metrics.accuracy_score(y_test,
                             model.predict(X_test))*100)

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Generate predictions with the best model
y_pred = clf.predict(X_test)
y_pred

# Create the confusion matrix
cm = confusion_matrix(y_test, y_pred)

#ConfusionMatrixDisplay(confusion_matrix=cm).plot()

sns.heatmap(cm,
            annot=True,
            fmt='g')
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)
plt.show()